{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# script: sec_form_d_lookup\n",
        "# author: Jennifer Lammers Zimmer, jamm@umich.edu\n",
        "# last updated: 01/27/2023\n",
        "# This script uses the Edgar Python package to query the SEC database to find\n",
        "# CIKs for companies in a CSV file. A lookup on names is performed first to find\n",
        "# the official Edgar name and then Edgar is queried again to find the CIKs with the \n",
        "# official names. The final output is parsed with Beautiful Soup and is then\n",
        "# is written to a CSV file and includes official company name and CIK.\n",
        "\n",
        "#Data fields requested from the Soup are:\n",
        "      #filed on date\n",
        "      #signed_date\n",
        "      #industry\n",
        "      #former_name(s)\n",
        "      #cik\n",
        "      #offical company name\n",
        "      #phone\n",
        "      #state of incorporation\n",
        "      #legal entity type\n",
        "      #address\n",
        "      #address2\n",
        "      #city\n",
        "      #state\n",
        "      #state full name \n",
        "      #zip,\n",
        "      #Officer names with roles\n",
        "\n",
        "#\n",
        "# Edgar is a Python Package to query the SEC Edgar Database\n",
        "# https://pypi.org/project/edgar/\n",
        "# \n",
        "# BeautifulSoup is a Python Package for parsing HTML and XML\n",
        "# https://beautiful-soup-4.readthedocs.io/en/latest/#"
      ],
      "metadata": {
        "id": "hfWZGQe0nyks"
      },
      "id": "hfWZGQe0nyks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4990de7c",
      "metadata": {
        "id": "4990de7c"
      },
      "outputs": [],
      "source": [
        "import lxml\n",
        "from lxml import etree\n",
        "import requests\n",
        "from io import StringIO, BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4f8dbe8",
      "metadata": {
        "id": "e4f8dbe8"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from bs4 import BeautifulStoneSoup\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37c98c1b",
      "metadata": {
        "id": "37c98c1b"
      },
      "outputs": [],
      "source": [
        "#code from https://pypi.org/project/edgar/#documents - edgar 5.4.3\n",
        "#get companies by name and cik number\n",
        "#get specific filing type/form (in our case Form D)\n",
        "#create a list of lxml.html.ElementTree documents, one for each requested. \n",
        "#This is a LIST of things!\n",
        "\n",
        "# Install and Call Edgar Package\n",
        "# note: pip install only needed if running on Colabs or you don't have Edgar installed\n",
        "# in your local environment already. \n",
        "\n",
        "!pip install Edgar\n",
        "\n",
        "from edgar import Company"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1.\n",
        "# GET COMPANY NAMES AND CIKS FROM CSV FILE\n",
        "# Use file generated from edgar_cik_company_lookup.ipynb or your own file.\n",
        "# Expected file layout is two columns with a header row\n",
        "# Eg:\n",
        "#      SEC_company,CIK\n",
        "#      EPIC THERAPEUTICS INC,0001131996 \n",
        "#      PHYTERA INC,0000917054\n",
        "#      C2 THERAPEUTICS INC,0001399463\n",
        "\n",
        "\n",
        "\n",
        "# Read in companies from a csv file to create a list of lists. Each list will contain one company name.\n",
        "# If running on Colabs, you will need to upload the file to the runtime to use it\n",
        "# or pull it from a Google Drive:\n",
        "# This section of code gets the data from my google drive\n",
        "#\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#f = open('/content/drive/MyDrive/Colab Notebooks/filename.csv', encoding=\"utf8\")\n",
        "#frs_text = f.read()\n",
        "#f.close()\n",
        "\n",
        "\n",
        "company_cik = []\n",
        "with open(\"SEC_companies_cik1.csv\", 'r') as file:\n",
        "    csvreader = csv.reader(file)\n",
        "    header = next(csvreader)\n",
        "    for row in csvreader:\n",
        "        company_cik.append(row)\n",
        "\n",
        "print(header)\n",
        "print(company_cik[30:40])\n"
      ],
      "metadata": {
        "id": "mwCS5jUIHKCd"
      },
      "id": "mwCS5jUIHKCd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13e02b9e",
      "metadata": {
        "id": "13e02b9e"
      },
      "outputs": [],
      "source": [
        "## STEP 2.\n",
        "# GET FORM D CONTENT\n",
        "# This code is from the edgar.py package. It takes in a company name and cik from the \n",
        "# company_cik list generated in Step 1 and passes it to the Edgar package for \n",
        "# querying at the SEC. The 'filing_type' can be set to any form name and the no_of_documents\n",
        "# tells the script how many forms for each company/ form type it should return.\n",
        "\n",
        "# The output is a list of lists of the forms as xml/html element trees which needs to be flattened \n",
        "# and then parsed for the data needed.\n",
        "\n",
        "# More information on the classes and methods for Edgar can be found at\n",
        "# https://pypi.org/project/edgar/\n",
        "#\n",
        "# The github repository for the code in the package is at \n",
        "# https://github.com/joeyism/py-edgar\n",
        "\n",
        "# Parameters you can change:\n",
        "#  filing_type\n",
        "#  no_of_documents\n",
        "\n",
        "docs = []\n",
        "for company,cik in company_cik:\n",
        "  print(company) #in case there is timeout error and the process quits we can know where we left off.\n",
        "  company = Company(company, cik)\n",
        "  tree = company.get_all_filings(filing_type = \"D\")\n",
        "  forms = Company.get_documents(tree, no_of_documents=20)\n",
        "  docs.append(forms)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For testing: take a look at the list\n",
        "#print(docs)"
      ],
      "metadata": {
        "id": "KLS6NIQUSJVn"
      },
      "id": "KLS6NIQUSJVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3669b18",
      "metadata": {
        "id": "a3669b18"
      },
      "outputs": [],
      "source": [
        "#Optional. You can choose a different parser to use when creating the Soup\n",
        "# create the parsers for working with the ElementTree documents\n",
        "#parser = etree.HTMLParser(remove_blank_text=True)\n",
        "#parserxml = etree.XMLParser(remove_blank_text=True, recover=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca46325a",
      "metadata": {
        "id": "ca46325a"
      },
      "outputs": [],
      "source": [
        "#STEP 3.\n",
        "#CREATE DOCUMENT LIST\n",
        "# unpack each item on the list of elementTrees into a string of code that you can read.\n",
        "# This will create a list of the full code of the documents, one document/form = one item in the list\n",
        "# These are still 'bytes' type items on the list even with the 'tostring' method. Don't ask me why.\n",
        "# This is the format needed for BeautifulSoup to do its magic.\n",
        "\n",
        "doclist = []\n",
        "for item in [item for sublist in docs for item in sublist]:\n",
        "    page = etree.tostring(item, pretty_print=True, encoding=\"UTF-8\")\n",
        "    doclist.append(page)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6f2466d",
      "metadata": {
        "id": "f6f2466d"
      },
      "outputs": [],
      "source": [
        "#OPTIONAL - check to make sure things got put in the list\n",
        "#len(doclist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "580518a6",
      "metadata": {
        "id": "580518a6"
      },
      "outputs": [],
      "source": [
        "#OPTIONAL - take a document or two from the list to make sure it looks like we think it should.\n",
        "# change the number in the brackets to see different forms. Create a range with a colon, e.g. [23:30]\n",
        "# NOTE: Python starts counting (indexing) with 0 so the first item in the list would be doclist[0].\n",
        "#print(doclist[26])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba36bd9f",
      "metadata": {
        "id": "ba36bd9f"
      },
      "outputs": [],
      "source": [
        "#STEP 4.\n",
        "#GET FORM D DATA INTO CSV\n",
        "# Here we take each form (page) in the list of forms (doclist), parse it and find the parts\n",
        "# we want, then write those parts out to a CSV file. \n",
        "#NOTE: the 'formD.csv' file is opened in APPEND mode so if you want to build one big output file,\n",
        "#you can keep running this with different doclists from your input csv files. \n",
        "\n",
        "with open('formD.csv', 'a') as csvfile:    \n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    for page in doclist:\n",
        "        result = []\n",
        "        xmlSoup = BeautifulStoneSoup(page)\n",
        "        #This bit weeds out the forms that aren't Form D that accidentally came along for the ride.\n",
        "        #Industry group is only in form D so we test for that. If it doesn't show, then\n",
        "        # the exception creates a list and writes a custom error message along with the page\n",
        "        # contents. \n",
        "        try:\n",
        "          industry = xmlSoup.industrygrouptype.string\n",
        "        except:\n",
        "          errornf = \"unknown form type\"\n",
        "          errors.append(errornf)\n",
        "          errors.append(str(xmlSoup))\n",
        "        #This bit actually reads through the page content and pulls out the bits we want \n",
        "        # based upon their xml tags.\n",
        "        else:\n",
        "          allNames = xmlSoup.findAll('relatedpersoninfo')\n",
        "          companyinfo_tag = xmlSoup.primaryissuer #grab the whole primary issuer\n",
        "          address_tag = xmlSoup.issueraddress #grab the address block\n",
        "          signed_date = xmlSoup.signaturedate.string\n",
        "          prev_names = xmlSoup.edgarpreviousnamelist #get all the previous names\n",
        "          \n",
        "          #industry = xmlSoup.industryGroupType.string\n",
        "\n",
        "          #get filed on date from the SEC Header block. This is not a part of the XML so it requires extra processing.\n",
        "          #This block is all text with line breaks, spaces and tabs. We have to break it up and look for the \n",
        "          #text line that contains the filed on date. If we don't find one, we create an error message to insert into the field.\n",
        "          accept_tag = xmlSoup.find(\"acceptance-datetime\")\n",
        "          if accept_tag is not None:\n",
        "              accept_list = accept_tag.text.split(\"\\n\")\n",
        "              match = \"\".join([str(s) for s in accept_list if \"FILED\" in s])\n",
        "          else:\n",
        "              match = \"No filing date found\"\n",
        "              signed_date = \"No signed date found\"\n",
        "          result.append(match)\n",
        "          result.append(signed_date)\n",
        "          result.append(industry) #we append industry here to make it show up some place usable in the csv\n",
        "\n",
        "          # Checking for previous names. If there are none, create a custom error message to insert\n",
        "          # into the csv.\n",
        "          try:\n",
        "            for child in prev_names.children:\n",
        "                child = (child.string)\n",
        "                if child != \"\\n\":\n",
        "                    result.append(child)\n",
        "          except: \n",
        "            errorpn = \"no previous names listed\"\n",
        "            result.append(errorpn)\n",
        "\n",
        "          #this bit looks through the primaryissuer block and grabs the items with no children and separates them into a field for \n",
        "          #the csv. It ignores new line characters which it will treat as their own string and you \n",
        "          #will end up with lots of empty fields (like hundreds, it filled up the sheet on the first go around).\n",
        "          for child in companyinfo_tag.children:\n",
        "              child = (child.string)\n",
        "              if child != \"\\n\":\n",
        "                  result.append(child)\n",
        "\n",
        "         #this bit looks through the address block and grabs the items with no children and separates them into a field for \n",
        "          #the csv. It ignores new line characters, which it will treat as their own string,  so you \n",
        "          #don't end up with lots of empty fields (like hundreds, it filled up the sheet on the first go around).\n",
        "          for child in address_tag.children:\n",
        "              child = (child.string)\n",
        "              if child != \"\\n\":\n",
        "                  result.append(child)\n",
        "          # This bit unpacks the data from the relatedpersoninfo block and creates a single name from \n",
        "          # the three fields. If middlename is empty, it enters a blank in the name.\n",
        "          # Then it looks through the relationships and creates a list of all relationships. Some\n",
        "          # people have more than one. The name and relationships gets entered into the csv as \n",
        "          # a list. This keeps the names and relationships together.\n",
        "          for name in allNames:\n",
        "              print(name)\n",
        "              person = []\n",
        "              relation = []\n",
        "              fn = (name.find('firstname').string)\n",
        "              if xmlSoup.middlename not in name:\n",
        "                  mn = \"\"\n",
        "              else:\n",
        "                  mn = (name.find('middlename').string)\n",
        "                  print(mn)\n",
        "              ln = (name.find('lastname').string)\n",
        "              rel_tag = name.relatedpersonrelationshiplist\n",
        "              for child in rel_tag.children:\n",
        "                  child = child.string\n",
        "                  if child != \"\\n\":\n",
        "                      relation.append(child.string)\n",
        "              person.append(ln+', '+fn+', '+mn+', '+', '.join(relation)) \n",
        "              result.append(person)\n",
        "\n",
        "          \n",
        "          # Here is where we write all the data requested for the current form to the csv.\n",
        "          writer.writerow(result)\n",
        "        \n",
        "        \n",
        "#always close your file!      \n",
        "csvfile.close()\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#STEP 5. OPTIONAL\n",
        "# CREATE ERRORS FILE\n",
        "# Write out the errors to a csv file for further inspection. There may be an actual Form D or two in here\n",
        "# due to malformed xml or missing data. I didn't come across any in the spot checking I did, but\n",
        "# better safe than sorry.\n",
        "\n",
        "#field_names2 = ['Error','DocCode']\n",
        "#create a CSV file for the ouput\n",
        "with open('errors.csv', 'a') as csvfile:\n",
        "      writer = csv.writer(csvfile)\n",
        "      #writer.writerow(field_names2)\n",
        "      for item in errors:\n",
        "        writer.writerow(item)\n",
        "\n",
        "              \n",
        "        \n",
        "#always close your file!        \n",
        "csvfile.close()"
      ],
      "metadata": {
        "id": "JcoGbkZsRdGj"
      },
      "id": "JcoGbkZsRdGj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}